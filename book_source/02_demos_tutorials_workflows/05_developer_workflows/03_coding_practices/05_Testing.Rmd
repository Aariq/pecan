### Testing {#developer-testing}

PEcAn uses the testthat package developed by Hadley Wickham. Hadley has
written instructions for using this package in his
[Testing](http://adv-r.had.co.nz/Testing.html) chapter.

#### Rationale

*  makes development easier
*  provides working documentation of expected functionality
*  saves time by allowing computer to take over error checking once a
    test has been made
*  improves code quality
*  Further reading: [Aruliah et al 2012 Best Practices for Scientific Computing](http://arxiv.org/pdf/1210.0530v3.pdf)

#### Tests makes development easier and less error prone

Testing makes it easier to develop by organizing everything you are
already doing anyway - but integrating it into the testing and
documentation. With a codebase like PEcAn, it is often difficult to get
started. You have to figure out

*  what was I doing yesterday?
*  what do I want to do today?
*  what existing functions do I need to edit?
*  what are the arguments to these functions (and what are examples of
    valid arguments)
*  what packages are affected
*  where is a logical place to put files used in testing

#### Quick Start:

* decide what you want to do today
* identify the issue in github (if none exists, create one)
* to work on issue 99, create a new branch called “github99” or some descriptive name… Today we will enable an
existing function, `make.cheas` to make `goat.cheddar`. We will know
that we are done by the color and taste.
  ```
  git branch goat-cheddar
  git checkout goat-cheddar
  ```
* open existing (or create new) file in `inst/tests/`. If working on code in "myfunction" or a set of functions in "R/myfile.R", the file should be named accordingly, e.g. "inst/tests/test.myfile.R"
* if you are lucky, the function has already been tested and has some examples.
* if not, you may need to create a minimal example, often requiring a settings file. The default settings file can be obtained in this way:
  ```r
  settings <- read.settings(system.file("extdata/test.settings.xml", package = "PEcAn.utils"))
  ```
* write what you want to do
  ```
  test_that("make.cheas can make cheese",{
    goat.cheddar <- make.cheas(source = 'goat', style = 'cheddar')
    expect_equal(color(goat.cheddar), "orange")
    expect_is(object = goat.cheddar, class = "cheese")
    expect_true(all(c("sharp", "creamy") %in% taste(goat.cheddar)))
  }
  ```
* now edit the goat.cheddar function until it makes savory, creamy, orange cheese.
* commit often
* update documentation and test
  ```{r, eval = FALSE}
  library(devtools)
  document("mypkg")
  test("mypkg")
  ```
* commit again
* when complete, merge, and push
  ```{bash, eval = FALSE}
  git commit -m "make.cheas makes goat.cheddar now"
  git checkout master
  git merge goat-cheddar
  git push
  ```

#### Test files


Many of PEcAn’s functions require inputs that are provided as data.
These can be in the `/data` or the `/inst/extdata` folders of a package.
Data that are not package specific should be placed in the PEcAn.all or
PEcAn.utils files.

Some useful conventions:

#### Settings

* A generic settings can be found in the PEcAn.all package
```r
settings.xml <- system.file("pecan.biocro.xml", package = "PEcAn.BIOCRO")
settings <- read.settings(settings.xml)
```

*  database settings can be specified, and tests run only if a connection is available

We currently use the following database to run tests against; tests that require access to a database should check `db.exists()` and be skipped if it returns FALSE to avoid failed tests on systems that do not have the database installed.

```r
settings$database <- list(userid = "bety", 
                          passwd = "bety", 
                          name = "bety",     # database name 
                          host = "localhost" # server name)
test_that(..., {
  skip_if_not(db.exists(settings$database))
  ## write tests here
})
```

*  instructions for installing this are available on the [VM creation
    wiki](VM-Creation.md)
*  examples can be found in the PEcAn.DB package (`base/db/tests/testthat/`).

* Model specific settings can go in the model-specific module, for
example:

```r
settings.xml <- system.file("extdata/pecan.biocro.xml", package = "PEcAn.BIOCRO")
settings <- read.settings(settings.xml)
```
* test-specific settings:
  - settings text can be specified inline:
    ```
    settings.text <- "
      <pecan>
        <nocheck>nope</nocheck> ## allows bypass of checks in the read.settings functions
        <pfts>
          <pft>
            <name>ebifarm.pavi</name>
            <outdir>test/</outdir>
          </pft>
        </pfts>
        <outdir>test/</outdir>
        <database>
          <userid>bety</userid>
          <passwd>bety</passwd>
          <location>localhost</location>
          <name>bety</name>
        </database>
      </pecan>"
    settings <- read.settings(settings.text)
    ```
  - values in settings can be updated:
    ```r
    settings <- read.settings(settings.text)
    settings$outdir <- "/tmp" ## or any other settings
    ```

#### Helper functions created to make testing easier

*  **tryl** returns FALSE if function gives error
*  **temp.settings** creates temporary settings file
*  **test.remote** returns TRUE if remote connection is available
*  **db.exists** returns TRUE if connection to database is available

#### When should I test?

A test *should* be written for each of the following situations:

1.  Each bug should get a regression test.
 * The first step in handling a bug is to write code that reproduces the error
 * This code becomes the test
 * most important when error could re-appear
 * essential when error silently produces invalid results

2.  Every time a (non-trivial) function is created or edited
 * Write tests that indicate how the function should perform
     * example: `expect_equal(sum(1,1), 2)` indicates that the sum
            function should take the sum of its arguments

 * Write tests for cases under which the function should throw an
        error
  * example: `expect_error(sum("foo"))`
  * better : `expect_error(sum("foo"), "invalid 'type' (character)")`

#### What types of testing are important to understand?


#### Unit Testing / Test Driven Development

Tests are only as good as the test

1.  write test
2.  write code

#### Regression Testing

When a bug is found,

1.  write a test that finds the bug (the minimum test required to make
    the test fail)
2.  fix the bug
3.  bug is fixed when test passes

#### How should I test in R? The testthat package.


tests are found in `~/pecan/<packagename>/inst/tests`, for example
`utils/inst/tests/`

See attached file and
[http://r-pkgs.had.co.nz/tests.html](http://r-pkgs.had.co.nz/tests.html)
for details on how to use the testthat package.

##### List of Expectations

|Full	|Abbreviation|
|---|----|
|expect_that(x, is_true())	|expect_true(x)|
|expect_that(x, is_false())	|expect_false(x)|
|expect_that(x, is_a(y))	|expect_is(x, y)|
|expect_that(x, equals(y))	|expect_equal(x, y)|
|expect_that(x, is_equivalent_to(y))	|expect_equivalent(x, y)|
|expect_that(x, is_identical_to(y))	|expect_identical(x, y)|
|expect_that(x, matches(y))	|expect_matches(x, y)|
|expect_that(x, prints_text(y))	|expect_output(x, y)|
|expect_that(x, shows_message(y))	|expect_message(x, y)|
|expect_that(x, gives_warning(y))	|expect_warning(x, y)|
|expect_that(x, throws_error(y))	|expect_error(x, y)|

##### How to run tests

add the following to “pecan/tests/testthat.R”

```r
library(testthat)
library(mypackage)

test_check("mypackage")
```

#### basic use of the testthat package

Here is an example of tests (these should be placed in
`<packagename>/tests/testthat/test-<sourcefilename>.R`:

```r
test_that("mathematical operators plus and minus work as expected",{
  expect_equal(sum(1,1), 2)
  expect_equal(sum(-1,-1), -2)
  expect_equal(sum(1,NA), NA)
  expect_error(sum("cat"))
  set.seed(0)
  expect_equal(sum(matrix(1:100)), sum(data.frame(1:100)))
})

test_that("different testing functions work, giving excuse to demonstrate",{
  expect_identical(1, 1)
  expect_identical(numeric(1), integer(1))
  expect_equivalent(numeric(1), integer(1))
  expect_warning(mean('1'))
  expect_that(mean('1'), gives_warning("argument is not numeric or logical: returning NA"))
  expect_warning(mean('1'), "argument is not numeric or logical: returning NA")
  expect_message(message("a"), "a")
})
```


##### Script testing

It is useful to add tests to a script during development. This allows
you to test that the code is doing what you expect it to do.

```r
* here is a fake script using the iris data set

test_that("the iris data set has the same basic features as before",{
  expect_equal(dim(iris), c(150,5))
  expect_that(iris$Sepal.Length, is_a("numeric"))
  expect_is(iris$Sepal.Length, "numeric")#equivalent to prev. line
  expect_is(iris$Species, "factor")
})

iris.color <- data.frame(Species = c("setosa", "versicolor", "virginica"),
                         color = c("pink", "blue", "orange"))

newiris <- merge(iris, iris.color)
iris.model <- lm(Petal.Length ~ color, data = newiris)

test_that("changes to Iris code occurred as expected",{
  expect_that(dim(newiris), equals(c(150, 6)))
  expect_that(unique(newiris$color),
              is_identical_to(unique(iris.color$color)))
  expect_equivalent(iris.model$coefficients["(Intercept)"], 4.26)
})
```


##### Function testing

Testing of a new function, `as.sequence`. The function and documentation
are in source:R/utils.R and the tests are in source:tests/test.utils.R.

Recently, I made the function `as.sequence` to turn any vector into a
sequence, with custom handling of NA’s:


```r
function(x, na.rm = TRUE){
  x2 <- as.integer(factor(x, unique(x)))
  if(all(is.na(x2))){
    x2 <- rep(1, length(x2))
  }
  if(na.rm == TRUE){
    x2[is.na(x2)] <- max(x2, na.rm = TRUE) + 1
  }
  return(x2)
}

```
    

The next step was to add documentation and test. Many people find it
more efficient to write tests before writing the function. This is true,
but it also requires more discipline. I wrote these tests to handle the
variety of cases that I had observed.

As currently used, the function is exposed to a fairly restricted set of
options - results of downloads from the database and transformations.

```r
test_that(“as.sequence works”;{
 expect_identical(as.sequence(c(“a”, “b”)), 1:2)
 expect_identical(as.sequence(c(“a”, NA)), 1:2)
 expect_equal(as.sequence(c(“a”, NA), na.rm = FALSE), c(1,NA))
 expect_equal(as.sequence(c(NA,NA)), c(1,1))
})
```

#### Testing the Shiny Server

Shiny can be difficult to debug because, when run as a web service, the R output is hidden in system log files that are hard to find and read.
One useful approach to debugging is to use port forwarding, as follows.

First, on the remote machine (including the VM), make sure R's working directory is set to the directory of the Shiny app (e.g., `setwd(/path/to/pecan/shiny/WorkflowPlots)`, or just open the app as an RStudio project).
Then, in the R console, run the app as:

```
shiny::runApp(port = XXXX)
# E.g. shiny::runApp(port = 5638)
```

Then, on your local machine, open a terminal and run the following command, matching `XXXX` to the port above and `YYYY` to any unused port on your local machine (any 4-digit number should work).

```
ssh -L YYYY:localhost:XXXX <remote connection>
# E.g., for the PEcAn VM, given the above port:
# ssh -L 5639:localhost:5638 carya@localhost -p 6422
```

Now, in a web browser on your local machine, browse to `localhost:YYYY` (e.g., `localhost:5639`) to run whatever app you started with `shiny::runApp` in the previous step.
All of the output should display in the R console where the `shiny::runApp` command was executed.
Note that this includes any `print`, `message`, `logger.*`, etc. statements in your Shiny app.

If the Shiny app hits an R error, the backtrace should include a line like `Hit error at of server.R#LXX` -- that `XX` being a line number that you can use to track down the error.
To return from the error to a normal R prompt, hit `<Control>-C` (alternatively, the "Stop" button in RStudio).
To restart the app, run `shiny::runApp(port = XXXX)` again (keeping the same port).

Note that Shiny runs any code in the `pecan/shiny/<app>` directory at the moment the app is launched.
So, any changes you make to the code in `server.R` and `ui.R` or scripts loaded therein will take effect the next time the app is started.

If for whatever reason this doesn't work with RStudio, you can always run R from the command line.
Also, note that the ability to forward ports (`ssh -L`) may depend on the `ssh` configuration of your remote machine.
These instructions have been tested on the PEcAn VM (v.1.5.2+).

#### Testing PEcAn in bulk

The [`base/workflow/inst/batch_run.R`][batch_run] script can be used to quickly run a series of user-specified integration tests without having to create a bunch of XML files.
This script is powered by the [`PEcAn.workflow::create_execute_test_xml()`][xml_fun] function,
which takes as input information about the model, meteorology driver, site ID, run dates, and others,
uses these to construct a PEcAn XML file,
and then uses the `system()` command to run a workflow with that XML.

If run without arguments, `batch_run.R` will try to run the model configurations specified in the [`base/workflow/inst/default_tests.csv`][default_tests] file.
This file contains a CSV table with the following columns:

- `model` -- The name of the model (`models.model_name` column in BETY)
- `revision` -- The version of the model (`models.revision` column in BETY)
- `met` -- The name of the meteorology driver source
- `site_id` -- The numeric site ID for the model run (`sites.site_id`)
- `pft` -- The name of the plant functional type to run. If `NA`, the script will use the first PFT associated with the model.
- `start_date`, `end_date` -- The start and end dates for the model run, respectively. These should be formatted according to ISO standard (`YYYY-MM-DD`, e.g. `2010-03-16`)
- `sensitivity` -- Whether or not to run the sensitivity analysis. `TRUE` means run it, `FALSE` means do not.
- `ensemble_size` -- The number of ensemble members to run. Set this to 1 to do a single run at the trait median.
- `comment` -- An string providing some user-friendly information about the run.

The `batch_run.R` script will run a workflow for every row in the input table, sequentially (for now; eventually, it will try to run them in parallel),
and at the end of each workflow, will perform some basic checks, including whether or not the workflow finished and if the model produced any output.
These results are summarized in a CSV table (by default, a file called `test_result_table.csv`), with all of the columns as the input test CSV plus the following:

- `outdir` -- Absolute path to the workflow directory.
- `workflow_complete` -- Whether or not the PEcAn workflow completed. Note that this is a relatively low bar -- PEcAn workflows can complete without having run the model or finished some other steps.
- `has_jobsh` -- Whether or not PEcAn was able to write the model's `job.sh` script. This is a good indication of whether or not the model's `write.configs` step was successful, and may be useful for separating model configuration errors from model execution errors.
- `model_output_raw` -- Whether or not the model produced any output files at all. This is just a check to see of the `<workflow>/out` directory is empty or not. Note that some models may produce logfiles or similar artifacts as soon as they are executed, whether or not they ran even a single timestep, so this is not an indication of model success.
- `model_output_processed` -- Whether or not PEcAn was able to post-process any model output. This test just sees if there are any files of the form `YYYY.nc` (e.g. `1992.nc`) in the `<workflow>/out` directory.

Right now, these checks are not particularly robust or comprehensive, but they should be sufficient for catching common errors.
Development of more, better tests is ongoing.

The `batch_run.R` script can take the following command-line arguments:

- `--help` -- Prints a help message about the script's arguments
- `--dbfiles=<path>`  -- The path to the PEcAn `dbfiles` folder. The default value is `~/output/dbfiles`, based on the file structure of the PEcAn VM. Note that for this and all other paths, if a relative path is given, it is assumed to be relative to the current working directory, i.e. the directory from which the script was called.
- `--table=<path>` -- Path to an alternate test table. The default is the `base/workflow/inst/default_tests.csv` file. See preceding paragraph for a description of the format.
- `--userid=<id>` -- The numeric user ID for registering the workflow. The default value is 99000000002, corresponding to the guest user on the PEcAn VM.
- `--outdir=<path>` -- Path to a directory (which will be created if it doesn't exist) for storing the PEcAn workflow outputs. Default is `batch_test_output` (in the current working directory).
- `--pecandir=<path>` -- Path to the PEcAn source code root directory. Default is the current working directory.
- `--outfile=<path>` -- Full path (including file name) of the CSV file summarizing the results of the runs. Default is `test_result_table.csv`. The format of the output

[batch_run]: https://github.com/pecanproject/pecan/tree/develop/base/workflow/inst/batch_run.R
[default_tests]: https://github.com/pecanproject/pecan/tree/develop/base/workflow/inst/default_tests.csv
[xml_fun]: 
